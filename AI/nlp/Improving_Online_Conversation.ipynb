{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Natural Language Processing (NLP) -  Working Copy\n",
    "\n",
    "### Improving online conversation: Use of NLP analysis to build a multi-headed model capable of detecting different types of online discussion toxicity like threats, obscenity, insults, and identity-based hate.\n",
    "\n",
    " * Blog post URL: https://andiosika.github.io/focal_loss_in_deep_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**WARNING:** The nature of this project comes with extremly offensive language.  The intention is to use analysis in order to mitigate situations where others feel unable to share their views at the risk of abuse or feeling that are in harm's way.  Please note that the language comes from an external source and does not represent personal language usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Quick Links:\n",
    "**Link** | **Description**\n",
    "--| --|\n",
    "[Background](#Background:) | Details around the subject, datasource and objective\n",
    "[Preprocessing](#Preprocessing:) | Steps taken to prepare data for modeling and evaluation\n",
    "[Visualizations](Visualizations:) |[Types of Toxicity by Occurance](#Visual:-Types-of-Toxic-Comments-by-Occurence:) , [Correlations Between Classes](#Visual:-Correlations-between-varying-forms-of-toxicity:) and others.\n",
    "[Main Dataset](#Main-Dataset:) | The dataset in it's final form used for the predictive modeling results described in the [Conclusion](#Conclusion:) section which splits ~150K samples into a [Train_Test_Split](#Train-Test-Split:) with 6 multi-label categories of in some cases extremely imbalanced sampling of targets.\n",
    "[Modeling](#Modeling:) | Various iterations of Neural Netorks including Recurrent Neural Networks using LSTM and BinomalFocal Loss - \n",
    "[Best Model](#Best-Model:-LSTM_50_Dense_25_Binary_Focal_Loss) | A Recurrent Neural Netork using Long Short Term Meory and Binomal Focal loss. \n",
    "[Conclusion](#Conclusion:) | Summation of outcomes from modeling\n",
    "[Recommendations](#Recommendations:)| Recommendations from findings as well as ideas for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#<img src='https://raw.githubusercontent.com/andiosika/dsc-mod-4-project-v2-1-online-ds-pt-100719/images/CleanWrdCloud'width=40%alignment=l>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freedom of speech is a right.  Digital platforms facilitate conversations, but struggle to do so efficiently.  While enabling this freedom, minimizing abuse and harrasment that can come with the 'anonymity effect' of a virtual climate can be challenging.  Those who feel threatened or abused could potentially not share their thoughts when considering the reprecussions thereby defeating the purpose of sharing altogether.  While freedom of speech exists, the law has long recognized specific limitations when it comes to to this freedom - such as prohibitions against slander and libel.  Detection of such language becomes valuable for providers in order to keep the conversation going in a way that is constructive in order for users to use such platforms in the way they were intended. \n",
    "\n",
    "This dataset is provided by [Conversation AI ](https://conversationai.github.io/) is a collaborative research effort exploring ML as a tool for better discussions online.  The source is a collection of comments from Wikipedia’s talk page edits circa 2017.  The result is a classification list of 159,571 samples provided by Wikipedia and have been labeled by human raters for toxic effects.  These comment classifications can fall into more than one of the following categories:\n",
    "\n",
    ">The types of toxicity are:\n",
    "* toxic\n",
    "- severe_toxic\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_hate\n",
    "\n",
    "\n",
    "**Warning:** the text found in each of the categories outlined above is extremely offensive.  The supporting visualizations are equally offensive.  Because of this, edited versions of some the most offensive findings were/are edited text that I feel conveys the ideas without perpetuating the toxic messages and used for the non-technical explanation.  However the findings in this analysis are necessary for identification's sake.  The intention is to use this type of analysis in order to mitigate situations where others feel unable to share their views at the risk of abuse or feeling that are in harm's way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import: Trainging and Testing DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test - DID NOT USE THE TESTING DATASET PROVIDED.. INSTEAD SPLIT THE TRAINING DATA TO MAKE THE OVERALL DATASET SMALLER DUE TO THE COMPUTATIONAL EXPENSE CAUSED BY LARGER DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001cee341fdb12</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000247867823ef7</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00013b17ad220c46</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017563c3f7919a</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017695ad8997eb</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text\n",
       "id                                                                 \n",
       "00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv.zip', index_col='id')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test = test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_test = pd.read_csv('test_labels.csv.zip', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001cee341fdb12</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000247867823ef7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00013b17ad220c46</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017563c3f7919a</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017695ad8997eb</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "id                                                                           \n",
       "00001cee341fdb12     -1            -1       -1      -1      -1             -1\n",
       "0000247867823ef7     -1            -1       -1      -1      -1             -1\n",
       "00013b17ad220c46     -1            -1       -1      -1      -1             -1\n",
       "00017563c3f7919a     -1            -1       -1      -1      -1             -1\n",
       "00017695ad8997eb     -1            -1       -1      -1      -1             -1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001ea8717f6de06</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000247e83dcc1211</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002f87b16116a7f</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003e1cccfd5a40a</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059ace3e3e9a53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "id                                                                           \n",
       "0001ea8717f6de06      0             0        0       0       0              0\n",
       "000247e83dcc1211      0             0        0       0       0              0\n",
       "0002f87b16116a7f      0             0        0       0       0              0\n",
       "0003e1cccfd5a40a      0             0        0       0       0              0\n",
       "00059ace3e3e9a53      0             0        0       0       0              0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test[y_test.sum(axis=1)!=-6]\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57888</td>\n",
       "      <td>63611</td>\n",
       "      <td>60287</td>\n",
       "      <td>63767</td>\n",
       "      <td>60551</td>\n",
       "      <td>63266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6090</td>\n",
       "      <td>367</td>\n",
       "      <td>3691</td>\n",
       "      <td>211</td>\n",
       "      <td>3427</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0  57888         63611    60287   63767   60551          63266\n",
       "1   6090           367     3691     211    3427            712"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_counts_test = y_test.apply(pd.Series.value_counts)\n",
    "val_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_counts_test.head(2).plot(kind='bar',figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/55975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[target_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108224</td>\n",
       "      <td>118490</td>\n",
       "      <td>113300</td>\n",
       "      <td>119312</td>\n",
       "      <td>113750</td>\n",
       "      <td>118621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11454</td>\n",
       "      <td>1188</td>\n",
       "      <td>6378</td>\n",
       "      <td>366</td>\n",
       "      <td>5928</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0  108224        118490   113300  119312  113750         118621\n",
       "1   11454          1188     6378     366    5928           1057"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_counts = y_train.apply(pd.Series.value_counts)\n",
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>119678.000000</td>\n",
       "      <td>119678.000000</td>\n",
       "      <td>119678.000000</td>\n",
       "      <td>119678.000000</td>\n",
       "      <td>119678.000000</td>\n",
       "      <td>119678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095707</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.053293</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.049533</td>\n",
       "      <td>0.008832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294190</td>\n",
       "      <td>0.099137</td>\n",
       "      <td>0.224618</td>\n",
       "      <td>0.055217</td>\n",
       "      <td>0.216979</td>\n",
       "      <td>0.093563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  119678.000000  119678.000000  119678.000000  119678.000000   \n",
       "mean        0.095707       0.009927       0.053293       0.003058   \n",
       "std         0.294190       0.099137       0.224618       0.055217   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  119678.000000  119678.000000  \n",
       "mean        0.049533       0.008832  \n",
       "std         0.216979       0.093563  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any 'null' comment\n",
    "no_comment = X_train.isnull().value_counts()\n",
    "len(no_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_counts = y_train.apply(pd.Series.value_counts)\n",
    "val_counts.head(2).plot(kind='bar',figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##develop the viz above to illustrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>108224</td>\n",
       "      <td>11454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>118490</td>\n",
       "      <td>1188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>113300</td>\n",
       "      <td>6378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>119312</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>113750</td>\n",
       "      <td>5928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>118621</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0      1\n",
       "toxic          108224  11454\n",
       "severe_toxic   118490   1188\n",
       "obscene        113300   6378\n",
       "threat         119312    366\n",
       "insult         113750   5928\n",
       "identity_hate  118621   1057"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts_train = round(val_counts.head(2).T)\n",
    "value_counts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unlabelled comments is  67\n"
     ]
    }
   ],
   "source": [
    "unlabelled_in_all = y_train[(y_train['toxic']!=1) & (y_train['severe_toxic']!=1) & (y_train['obscene']!=1) & \n",
    "                            (y_train['threat']!=1) & (y_train['insult']!=1) & (y_train['identity_hate']!=1)]\n",
    "print('Percentage of unlabelled comments is ', round(len(unlabelled_in_all)/len(train)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in X_train is 119678\n"
     ]
    }
   ],
   "source": [
    "#Total rows in train\n",
    "print('Total rows in X_train is {}'.format(len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toxic_classes = y_train[y_train[target_classes].sum(axis=1)>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52481</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20943</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145315</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147450</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35349</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53658</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22968</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142866</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149489</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12147 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "52481       1             0        0       0       0              0\n",
       "20943       1             0        1       0       1              0\n",
       "5874        1             0        0       0       1              0\n",
       "145315      1             0        1       0       1              0\n",
       "147450      1             0        1       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "35349       1             1        1       0       1              0\n",
       "53658       1             0        0       0       0              0\n",
       "22968       1             0        0       0       0              0\n",
       "142866      1             0        1       0       1              1\n",
       "149489      1             0        1       0       1              0\n",
       "\n",
       "[12147 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that some labels can be assigned to more than one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9429488762657446"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_classes['toxic'].sum()/len(toxic_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... but not all comments tagged are necessarily 'toxic', but most of them are... 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distributions:\n",
    "\n",
    "An initial look will examine frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = str(list(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing initial set of text corpus to develop stopword list:\n",
    "(Takes 2 Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-10 04:07:57.788375+03:00\n",
      "[i] Timer started at04/10/22 - 04:07 AM\n",
      "[i] Timer ended at 04/10/22 - 04:07 AM\n",
      "- Total time = 0:00:00.000023\n"
     ]
    }
   ],
   "source": [
    "import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Timer started at04/10/22 - 04:07 AM\n"
     ]
    }
   ],
   "source": [
    "timer = fn.Timer()\n",
    "\n",
    "timer.start()\n",
    "tokens = word_tokenize(text)\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "#remove punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "##adding adhoc all strings that don't appear to contribute, added 'article, page and wikipedia' iteratively as \n",
    "##these are parts of most comment strings\n",
    "stopwords_list += (\"''\",\"``\", \"'s\", \"\\\\n\\\\n\" , '...', 'i\\\\','\\\\n',\n",
    "                   '•', \"i\", 'the', \"'m\", 'i\\\\', \"'ve\", \"don\\\\'t\",\n",
    "                  \"'re\", \"\\\\n\\\\ni\", \"it\\\\\", \"'ll\", 'you\\\\', \"'d\", \"n't\",\n",
    "                  '’', 'article', 'page', 'wikipedia') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = fn.Timer()\n",
    "timer.start()\n",
    "stopped_tokens = [w.lower() for w in tokens if w.lower() not in stopwords_list]\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output =[lemmatizer.lemmatize(w) for w in stopped_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "freqdist = FreqDist(stopped_tokens)\n",
    "most_common_stopped = freqdist.most_common(200)\n",
    "most_common_stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_vals = freqdist.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feq_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdistlem = FreqDist(lemmatized_output)\n",
    "most_common_lem = freqdistlem.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_common_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing gave not much of a  different result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Looking Deeper at the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the training set to explore/manipulate.\n",
    "\n",
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=df.iloc[:,2:].sum()\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual: Types of Toxic Comments by Occurence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x.plot(kind='bar', figsize=(8,5))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Types of Toxic Comments by Occurence',fontsize=18 )\n",
    "plt.ylabel('Frequncy of Tag', fontsize=14)\n",
    "plt.xlabel('Category of Toxic Text', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxic is by far the highest occurance, and threats are significantly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Varrying Toxic Text and The Correlations:\n",
    "\n",
    "***Warning - offensive language** this was text that was imported from the dataset and has not been modified.  As a reminder the point of this excercise is to identify types of toxic language so it is necessary to see samples of each to look for patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simple_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from simple_colors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blue(\"Example of toxic text:\",['bold']))\n",
    "print(train[train['toxic']==1].iloc[2,1])\n",
    "print('-----------------------------')\n",
    "print(blue(\"Example of severe toxic text:\",['bold']))\n",
    "print(train[train['severe_toxic']==1].iloc[3,1])\n",
    "print('-----------------------------')\n",
    "print(blue(\"Example of obscene text:\",['bold']))\n",
    "print(train[train['obscene']==1].iloc[4,1])\n",
    "print('-----------------------------')\n",
    "print(blue(\"Example of threatening text:\",['bold']))\n",
    "print(train[train['threat']==1].iloc[2,1])\n",
    "print('-----------------------------')\n",
    "print(blue(\"Example of insulting text:\",['bold']))\n",
    "print(train[train['insult']==1].iloc[3,1])\n",
    "print('-----------------------------')\n",
    "print(blue(\"Example of identity hate based text:\",['bold']))\n",
    "print(train[train['identity_hate']==1].iloc[39,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual: Correlations between varying forms of toxicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=y_train.corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, cmap=\"Blues\", mask=mask, vmin=0, vmax=.95,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, annot=True)\n",
    "plt.title('Correlations Between Varying Levels of Toxicity', fontsize=(16))\n",
    "plt.xlabel('Type of Toxicity (for comparison on y-axis)',fontsize=(12), color='darkblue')\n",
    "plt.ylabel('Type of Toxicity (for comparison on x-axis)',fontsize=(12), color='darkblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the totals from each row to identify if they had any toxic comments or not\n",
    "rowtotals = train.iloc[:,2:].sum(axis=1)\n",
    "df['clean_text'] = (rowtotals==0)\n",
    "df['toxic_text'] = (rowtotals!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of clean comments:\", df['clean_text'].sum())\n",
    "print(\"Number of toxic comments:\", df['toxic_text'].sum())\n",
    "print(\"Percentage of toxic comments in sample\", (df['toxic_text'].sum()/\n",
    "                                                      (df['clean_text'].sum() +\n",
    "                                                       df['toxic_text'].sum()))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wrd_count'] = df['comment_text'].str.count(' ') + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual: Toxic Comment Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxic_text'].value_counts().plot(kind='bar')\n",
    "plt.title('Toxic Comment Frequency',fontsize=16)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel(\"Toxic Comments\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tox_text_df = df.loc[df['toxic_text']==1]\n",
    "clean_text_df = df.loc[df['clean_text']==1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking to see if ID has some sort of meaning... e.g. user id each value is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tox_text_df['id'].value_counts(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the strings to set up for counting unique numbers:\n",
    "tox_unique = set()\n",
    "tox_text_df['text'] = tox_text_df['comment_text'].str.lower().str.split().apply(tox_unique.update)\n",
    "clean_unique = set()\n",
    "clean_text_df['text'] = clean_text_df['comment_text'].str.lower().str.split().apply(clean_unique.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique words in toxic corpus',len(tox_unique))\n",
    "print(f'Number of unique words in clean corpus', len(clean_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far less toxic words, it makes sense there are fewer unique words in the toxic corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual: Word Count Comparison Toxic vs NonToxic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(tox_text_df['wrd_count'], color='yellow', label='Toxic Text')\n",
    "sns.distplot(clean_text_df['wrd_count'],color='lightgreen', label='Clean Text')\n",
    "plt.title('Word Count Comparison', fontsize = 18)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Word Count', fontsize=14)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: at a high level, there's not much difference comparatively in terms of word count between toxic and non toxic comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)1111111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating separate dataframes for each type of toxicity for possible analysis\n",
    "ToxicDf = df[(df['toxic'] == 1)]\n",
    "SevTox = df[(df['severe_toxic'] == 1)]\n",
    "Obs = df[(df['obscene'] == 1)]\n",
    "Threat = df[(df['threat'] == 1)]\n",
    "Inst = df[(df['insult'] == 1)]\n",
    "IDH8 = df[(df['identity_hate']==1)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#since toxic was highly correlated with obscene, looking into frequency:\n",
    "tox_obs = df[(df['toxic'] ==1) & (df['obscene'])]\n",
    "len(tox_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comments tagged as toxic:\", (len(ToxicDf)))\n",
    "print(\"Comments tagged as obscene:\", (len(Obs)))\n",
    "print(\"Comments tagged as toxic and obscene\", len(tox_obs))\n",
    "print(\"Percentage of comments tagged as toxic and obscene\" )\n",
    "print(\"compared to soley toxic\", (round(len(tox_obs)/len(Obs),2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text specficially classified for each class\n",
    "toxic_text = str(list(ToxicDf['comment_text']))\n",
    "sev_toxic_text = str(list(SevTox['comment_text']))\n",
    "obs_text = str(list(Obs['comment_text']))\n",
    "threat_text = str(list(Threat['comment_text']))\n",
    "insult_text = str(list(Inst['comment_text']))\n",
    "idh8_text = str(list(IDH8['comment_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(threat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Classes of Toxic Text:\n",
    "\n",
    "Warning: the hidden cells contain extremely toxic text that was collected in the samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Toxic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tox_toks = fn.tok_text(toxic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "type(tox_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_tox = fn.freq_dist(tox_toks)\n",
    "dict(freq_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reload(fn)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_tox_list = fn.clean_up(freq_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_toxic_df = pd.DataFrame(freq_tox_list, columns=['word', 'frequency'])\n",
    "freq_tox_top_5 = freq_toxic_df[0:4]\n",
    "freq_tox_top_5\n",
    "\n",
    "freq_tox_top_5.plot(kind='barh',x='word', y='frequency')\n",
    "plt.title('Top 5 Toxic Words', fontsize=14)\n",
    "plt.xlabel('frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common toxic word cloud is extrememly offensive - measures were taken to edit for the sake of observing without perpetuating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wordcloud.generate_from_frequencies(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary of primary offensive words to clean up a visual.  These are highly offensive and\n",
    "#I am completely uncomfortable even typing them for this purpose, but needs to be done.  Also symbols don't \n",
    "#render in a word cloud\n",
    "dic = {'fuck': 'fword', 'nigger' : \"nword\" , 'nigga':'nwordderivatie','fucking' : 'fwordderivative',\n",
    "               'faggot':'offensivewordforgayman', 'cunt' : 'cword' ,'cunts' : 'cwords', 'shit': 'shword', 'fag' : 'offensivewordforgayman',\n",
    "               \"'fuck\" : \"'fword'\", 'faggots':'fwordforgaymen'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_tox_toks = fn.replace_all(tox_toks, dic )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the nature and severity of the language, one word cloud will be used to demonstrate common words in the toxic classification:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "fn.wrd_cld(clean_tox_toks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Severe Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sev_tox_toks = fn.tok_text(sev_toxic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_sev_tox = fn.freq_dist(sev_tox_toks)\n",
    "freq_sev_tox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_sevtox_list = fn.clean_up(freq_sev_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_sevtoxic_df = pd.DataFrame(freq_sevtox_list, columns=['word', 'frequency'])\n",
    "freq_sevtox_top_5 = freq_sevtoxic_df[0:4]\n",
    "freq_sevtox_top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_sevtox_top_5.plot(kind='barh',x='word', y='frequency')\n",
    "plt.title('Top 5 Severely Toxic Words', fontsize=14)\n",
    "plt.xlabel('frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "obs_toks = fn.tok_text(obs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "obs_tox = fn.freq_dist(obs_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "obs_tox_list = fn.clean_up(obs_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_obs_df = pd.DataFrame(obs_tox_list, columns=['word', 'frequency'])\n",
    "freq_obs_top_5 = freq_obs_df[0:4]\n",
    "freq_obs_top_5.plot(kind='barh',x='word', y='frequency')\n",
    "plt.title('Top 5 Obscene Words', fontsize=14)\n",
    "plt.xlabel('frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "threat_toks = fn.tok_text(threat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "threat_tox = fn.freq_dist(threat_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_threat_list = fn.clean_up(threat_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_threat_df = pd.DataFrame(freq_threat_list, columns=['word', 'frequency'])\n",
    "freq_threat_top_5 = freq_threat_df[0:4]\n",
    "freq_threat_top_5.plot(kind='barh',x='word', y='frequency')\n",
    "plt.title('Top 5 Threatening Words', fontsize=14)\n",
    "plt.xlabel('frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "inslt_toks = tok_text(insult_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inslt_tox = freq_dist(inslt_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inslt_tox_list = clean_up(inslt_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_insult_df = pd.DataFrame(inslt_tox_list, columns=['word', 'frequency'])\n",
    "freq_insult_top_5 = freq_insult_df[0:4]\n",
    "freq_insult_top_5.plot(kind='barh',x='word', y='frequency')\n",
    "plt.title('Top 5 Insulting Words', fontsize=14)\n",
    "plt.xlabel('frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idh8_toks = fn.tok_text(idh8_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idh8_tox = fn.freq_dist(idh8_toks)\n",
    "idh8_tox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_idh8_list = fn.clean_up(idh8_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_idh8_df = pd.DataFrame(freq_idh8_list, columns=['word', 'frequency'])\n",
    "freq_idh8_top_5 = freq_idh8_df[0:4]\n",
    "freq_idh8_top_5.plot(kind='barh',x='word', y='frequency')\n",
    "plt.title('Top 5 Identity-Hate Based Words', fontsize=14)\n",
    "plt.xlabel('frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "#https://github.com/learn-co-students/dsc-classification-with-word-embeddings-codealong-online-ds-pt-100719\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "#try texts_to_matrix\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [len(comment) for comment in list_tokenized_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comments,bins = np.arange(0,500,10))\n",
    "plt.axvline(x=400, color='g', ls=':')\n",
    "plt.title('Comment Length of Tokenized Data')\n",
    "plt.xlabel('Length of Tokenized Data')\n",
    "plt.ylabel('Number of Occurances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#The majority distribution of comment length is ~0-30 words ... will capture everything with 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_t.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(X_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Neural Network: 10, 6 Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_model.add(Dense(10, activation='relu', input_shape=(X_t.shape[1],) ))\n",
    "van_model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_history = van_model.fit(X_t, y_train, epochs=10,\n",
    "                            batch_size=200,\n",
    "                            validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.plot_acc_loss(van_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h_van_probs = van_model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_van = pd.DataFrame(np.round(y_h_van_probs), columns=target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_rept = fn.class_report_model(y_train, y_test, y_preds_van)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla Neural Network Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialized a very small and shallow 'vanilla' model to observe convergence trends and behavior.  Zero response from validation data and model overtrains by second epoch.  While accuracy is reported 90%+, recall is 0 in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: LSTM_60_50_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing LSTM and dropout to see if there's improvement.:\n",
    "⏰**WARNING: Run Time is 1 Hour 45 Min**⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the callbacks, changing from 3-->2 since overfitting happens rapidly with subsequent modeling\n",
    "early_stopping = [EarlyStopping(monitor='val_loss', patience=2), \n",
    "                  ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn.add(Embedding(max_features, embedding_size, input_length=X_t.shape[1] ))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn.add(LSTM(60, return_sequences=True, name='lstm_layer'))\n",
    "rnn.add(GlobalMaxPool1D())\n",
    "rnn.add(Dropout(0.1))  \n",
    "rnn.add(Dense(20, activation='relu',kernel_regularizer=regularizers.l2(.0001) ))\n",
    "rnn.add(Dropout(0.1))\n",
    "rnn.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING ⏰ 1 Hr 20 min+ RunTime ⏰\n",
    "#fit the model\n",
    "timer = fn.Timer()\n",
    "timer.start()\n",
    "history = rnn.fit(X_t, y_train, epochs=20, \n",
    "                  callbacks=early_stopping,\n",
    "                  batch_size=300, validation_split=0.33)\n",
    "timer = timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.plot_acc_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Outcomes:\n",
    "\n",
    "To evaluate the training data, 'taking a look at how the model predictions work on training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h_train_probs = rnn.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba = rnn.predict(X_te)\n",
    "y_hat_test_proba[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pd.DataFrame(np.round(y_hat_test_proba), columns=target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_train = pd.DataFrame(np.round(y_h_train_probs), columns=target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_counts_y_preds = y_preds.apply(pd.Series.value_counts)\n",
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the training data (is different  from use of class_report_model function so need to modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,y_train.shape[1]):\n",
    "    y_i_hat_t = y_preds_train.iloc[:,i]\n",
    "    y_i_t = y_train.iloc[:,i]\n",
    "    print(y_train.columns[i])\n",
    "    print(classification_report(y_i_hat_t, y_i_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on RNN LSTM_60_50_6\n",
    "\n",
    "(See Below)Overall accuracy is 98%: Recall improved:\n",
    "\n",
    ".68 Toxic\n",
    ".13 Severe Toxic\n",
    ".76 Obscene\n",
    ".00 Threat\n",
    ".61 Insult\n",
    ".01 Identity-Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.plot_acc_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.class_report_model(y_train, y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix(y_i_hat,y_i_t, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to balance the classes to improve predictability using SMOTE\n",
    "\n",
    "This failed due to multi-label classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "# x_train_res, y_train_res = sm.fit_sample(X_t,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features = 2000\n",
    "# smote_tokenizer = Tokenizer(num_words=max_features)\n",
    "# smote_tokenizer.fit_on_texts(list(X_train))\n",
    "# #try texts_to_matrix\n",
    "# list_tokenized_train_res = tokenizer.texts_to_sequences(x_train_res)\n",
    "# list_tokenized_test_res = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen = 400\n",
    "# X_t_sm = pad_sequences(list_tokenized_train_res, maxlen=maxlen)\n",
    "# X_te_sm = pad_sequences(list_tokenized_test_res, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fundamental failure by design.. of smoting tokenized padded data abandoning...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovered **'Focal_loss'** as a means to attempt to offset imbalance. https://pypi.org/project/focal-loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install focal-loss\n",
    "from focal_loss import BinaryFocalLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN_LSTM_tiny_10_5_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_tiny = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn_tiny.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_tiny.add(LSTM(12, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_tiny.add(GlobalMaxPool1D())\n",
    "rnn_tiny.add(Dropout(0.1))  \n",
    "rnn_tiny.add(Dense(10, activation='relu',kernel_regularizer=regularizers.l1(.0001) ))\n",
    "rnn_tiny.add(Dropout(0.1))\n",
    "rnn_tiny.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_tiny.compile(loss=BinaryFocalLoss(gamma=2),\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## WARNING ⏰ 30 min RunTime ⏰\n",
    "#fit the model\n",
    "timer = fn.Timer()\n",
    "timer.start()\n",
    "history_tiny_rnn = rnn_tiny.fit(X_t, y_train, epochs=20, \n",
    "                       batch_size=200, \n",
    "                       callbacks=early_stopping,\n",
    "                       validation_split=0.25)\n",
    "timer = timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history_tiny_rnn.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.plot_acc_loss(history_tiny_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_proba_tiny = rnn_tiny.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_tiny = pd.DataFrame(np.round(y_hat_proba_tiny), columns=target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.class_report_model(y_train, y_test, y_preds_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "\n",
    "Overall accuracy - 98%\n",
    "Decreased performance with recall:\n",
    "        toxic : .58\n",
    "        severe toxic: 0 \n",
    "        obscene: .67\n",
    "        threat: 0\n",
    "        insult: .57\n",
    "        identity_hate: 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN_small_20_10_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_small = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn_small.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_small.add(LSTM(20, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_small.add(GlobalMaxPool1D())\n",
    "rnn_small.add(Dropout(0.1))  \n",
    "rnn_small.add(Dense(10, kernel_regularizer=regularizers.l2(.0001),activation='relu'))\n",
    "rnn_small.add(Dropout(0.1))\n",
    "rnn_small.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ##had binary crossentropy\n",
    "rnn_small.compile(loss=BinaryFocalLoss(gamma=2),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING ⏰ 30 Min+ RunTime ⏰\n",
    "#fit the model\n",
    "timer = fn.Timer()\n",
    "timer.start()\n",
    "history_small = rnn_small.fit(X_t, y_train, epochs=20, batch_size=300, \n",
    "                        callbacks=early_stopping, validation_split=0.2)\n",
    "timer = timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn.plot_acc_loss(history_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hat_test_proba_small = rnn_small.predict(X_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_small = pd.DataFrame(np.round(y_hat_test_proba_small), columns=target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.class_report_model(y_train, y_test, y_preds_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model:   LSTM_50_Dense_25_Binary_Focal_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together: LSTM_50_Dense_25_Binary_Focal_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 1\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "# tf.compat.v1.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks\n",
    "early_stopping = [EarlyStopping(monitor='val_loss', patience=2), \n",
    "                  ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install focal-loss\n",
    "from focal_loss import BinaryFocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_last = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn_last.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_last.add(LSTM(50, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_last.add(GlobalMaxPool1D())\n",
    "rnn_last.add(Dropout(0.2))  \n",
    "rnn_last.add(Dense(25, kernel_regularizer=regularizers.l2(.0001),activation='relu'))\n",
    "rnn_last.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_last.compile(loss=BinaryFocalLoss(gamma=2),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## WARNING ⏰ 70 Min+ RunTime ⏰\n",
    "#fit the model\n",
    "timer = fn.Timer()\n",
    "timer.start()\n",
    "history_lst = rnn_last.fit(X_t, y_train, epochs=20,  batch_size=300, \n",
    "                        callbacks=early_stopping, validation_split=0.3)\n",
    "timer = timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_last.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn.plot_acc_loss(history_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba_last = rnn_last.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_last = pd.DataFrame(np.round(y_hat_test_proba_last), columns=target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.class_report_model(y_train, y_test, y_preds_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of the above plus another layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_lst_deep = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn_lst_deep.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_lst_deep.add(LSTM(60, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_lst_deep.add(GlobalMaxPool1D())\n",
    "rnn_lst_deep.add(Dropout(0.1))  \n",
    "rnn_lst_deep.add(Dense(50, kernel_regularizer=regularizers.l2(.00001),activation='relu'))\n",
    "rnn_lst_deep.add(Dropout(.01))\n",
    "rnn_lst_deep.add(Dense(10, kernel_regularizer=regularizers.l2(.00001),activation='relu'))\n",
    "rnn_lst_deep.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_lst_deep.compile(loss=BinaryFocalLoss(gamma=2),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING ⏰ 21 Min+ RunTime ⏰\n",
    "#fit the model\n",
    "timer = fn.Timer()\n",
    "timer.start()\n",
    "history_lst_deep = rnn_lst_deep.fit(X_t, y_train, epochs=20, batch_size=300, \n",
    "                        callbacks=early_stopping, validation_split=0.3)\n",
    "timer = timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba_lst_deep = rnn_lst_deep.predict(X_te)\n",
    "y_preds_lst_deep = pd.DataFrame(np.round(y_hat_test_proba_lst_deep), columns=target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.plot_acc_loss(history_lst_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.class_report_model(y_train, y_test,y_preds_lst_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *goal* of this prjoect was to use NLP analysis to build a multi-headed model capable of detecting different types of online discussion toxicity like threats, obscenity, insults, and identity-based hate.\n",
    "\n",
    "Since I had little experience with deep learning, that was the method used to classify to gain experience.  It proved to be a challenging problem.\n",
    "\n",
    "There was signficant target imbalance of 10% being amplified needing to categorize 6 overlapping classes.  A search indicates that that this problem exists even for experts in the field.  One example can be found here where [researchers at Stanford](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6837517.pdf) experimented with this problem and still  seek ways to overcome this challenge.  \n",
    "\n",
    "\n",
    "\n",
    "**Preprocessing:** \n",
    "\n",
    "In evaluating the toxic text/comments to identify what classified them as such, patterns developed that demonstrated specific words helped in identifying toxicity. In particular, the same three words appared between 75%-100% of the time in four of the categories: Toxic, Severe Toxic, Insulting, and Obscene.  More unique words were used in threatening comments and identity-based hate comments.  Comment length was also comparitvely evaluated. It was determined that there was little difference in comment lenght between those made with toxic sentiment and those without. \n",
    "\n",
    "Also it was observed that there was major imbalance in the classes severe toxic, threat, and identitiy hate.  The table below illustrates this.  If a row has a 0 it indicates no target was identified.  A row with a 1 indicates a target was identified:\n",
    "\n",
    "| Target | Toxic |Severe_Toxic| Obscene\t| Threat|\tInsult |\tIdentity_Hate |\n",
    "|--| ---   | ---        | ---      | ---   | ---      | ---              |    \n",
    "| 0\t|108,224\t|118,490|\t113,300\t|119,312\t|113,750\t|118,621 |\n",
    "|1\t|11,454\t|1,188\t|6378\t|366| 5928\t|1057|\n",
    "\n",
    "Several Neural Netowrks were modeled using Keras: https://keras.io/\n",
    "\n",
    "While varying deep learning models were able to have high rates of accuracy their ability to identify toxic text proved to be more simple than the case of identifying the extremely underclassified examples, with an accuracy of rate of 98% , recall topped out at ~77% in the best case in the multi-class model using deep learning for one category.  In most cases, recall was 0 for the smaller classes.  \n",
    "\n",
    "**An initial and very shallow model was used as a baseline with extremely poor outcomes.** This model had a dense input layer of 10 and output of 6 to map to the varying classifiers. Using 'relu' activator on the main layer, and sigmoid on the exit, along with binary cross entropy as a loss function and stochastic gradient descent or sdg as the  optimizer.  \n",
    "Despite having an accuracy rate of 99% recall in all cases was 0 for being able to classify a true postive - or effectively classify anything.\n",
    "\n",
    "Since accuracy was high but the model seemed to not be able to identify true positives accuarately the *key metric identified to evaluate the model would be recall - specifically around True Positives*\n",
    "\n",
    "A Recurrent Neural Network was implemented using an embedding layer of 128 and LSTM Long Short Term Memory and dropout to see if improvement could be made.  In addition a l2 regulizer was added with a lambda of .0001.  This LSTM input layer with 60 neurons was followed by a MaxPool1 D layer and a dropout rate of 10% followed subsequently.  This model added a hidden layer and dropped the neurons in the hidden layer to 20, followed by an additional layer of 10% dropout. The output layer was the same as the first model with sigmoid activation.  However this time an 'adam' optimizer replaced the 'sdg'.  In this case, recall improved in the case of toxic (.68), obscene(.76), and insults(.61) classification. However in the cases of severe toxic(.13) it was extrememly low and threatening and identity-based hate comments, all recall was zero.  One could attribute this to extreme class-imbalance.  \n",
    "\n",
    "**3 - Layer RNN with a pattern 60_50_6 : Recall for all True Negatives was .99 - 1.0**\n",
    "\n",
    "|Classification | Recall|\n",
    "|-- | --|\n",
    "|Toxic | .68\n",
    "|Severe Toxic | .13\n",
    "|Obscene | .76\n",
    "|Threatening | .0\n",
    "|Insult | .61\n",
    "|Identity Hate | .01 \n",
    "\n",
    "\n",
    "Focal loss was implemented on varying sized neural networks with the same number of layers with similar outcomes.\n",
    "\n",
    "**3 - Layer RNN with a pattern 10_5_6 : Recall for all True Negatives was .99 - 1.0**\n",
    "\n",
    "|Classification | Recall|\n",
    "|-- | --|\n",
    "|Toxic | .65\n",
    "|Severe Toxic | .0\n",
    "|Obscene | .73\n",
    "|Threatening | .0\n",
    "|Insult | .61\n",
    "|Identity Hate | .0 \n",
    "\n",
    "\n",
    "**3 - Layer RNN with a pattern 20_10_6 : Recall for all True Negatives was .99 - 1.0**\n",
    "\n",
    "|Classification | Recall|\n",
    "|-- | --|\n",
    "|Toxic | .69\n",
    "|Severe Toxic | .0\n",
    "|Obscene | **.77**\n",
    "|Threatening | .0\n",
    "|Insult | .65\n",
    "|Identity Hate | .0 \n",
    "\n",
    "**3 - Layer RNN with a pattern 50_25_6 : Recall for all True Negatives was .99 - 1.0**\n",
    "\n",
    "|Classification | Recall|\n",
    "|-- | --|\n",
    "|Toxic | **.69**\n",
    "|Severe Toxic | **.32**\n",
    "|Obscene | .75\n",
    "|Threatening | .0\n",
    "|Insult | **.66**\n",
    "|Identity Hate | **.01** \n",
    "\n",
    "\n",
    "Despite having an overall lower recall this model is considered the best model because it was able to predict Severe Toxic comments 32% of the time which is better than any other model.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758\n",
    "#https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "#https://pypi.org/project/focal-loss/#:~:text=TensorFlow%20implementation%20of%20focal%20loss%20%5B1%5D%3A%20a%20loss%20function,losses%20functions%20and%20classes%2C%20respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's recommended to use this model to find overall toxic language and develop metrics and associated action plans to mitigate results of such language usage.  This work provides a foundation for future work on this subject matter.\n",
    "\n",
    "1. Implement model to identify varying levels of toxicity, perhaps simplify categories.\n",
    "\n",
    "\n",
    "2. Develop metrics around varying language  toxicty - Building on the more simplified model approach a suggestion or example could be: \n",
    "    * Level 1: could combine the four categories with the most common words: Toxic, Severe Toxic, Insulting, and Obscene \n",
    "    * Level 2: Identity Hate and \n",
    "    * Level 3: Threat  and decide outcomes for detection of each. \n",
    "    \n",
    "\n",
    "3. Continue to research an implement best practices.\n",
    "\n",
    "\n",
    "4.  Invest in Future Work - see below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work could be done in building out this model as well as modeling the more severe types of toxic comments - especially threats, as well as diving perhaps in to sentence structures to capture sentiment.\n",
    "\n",
    "Thankfully the most severe types of toxic comments are less frequent.  Despite the occurance of these severe cases, threats are serious.  In some situations, speech can constitute a crime, such as in the case of criminal threats. A criminal threat, sometimes known as the terrorist threat, malicious harassment, or by other terms, occurs when someone threatens to kill or physically harm someone else with intent and specificity.  https://www.fbi.gov/audio-repository/news-podcasts-gotcha-prison-time-for-making-threats-over-the-internet-ii.mp3/view. It's recommended to collect more data around these more severe types of toxic comments to improve recognition. \n",
    "\n",
    "Perhaps modeling could be done to classify three main categories: Toxic- which would encapsulate toxic, severely toxic, insulting and obscene; Threatening and Identity - they could be single category classifiers. \n",
    "\n",
    "As mentioned before, the most obvious toxic words are fairly easy to classify.  However sometimes when looking closely at single words, sentiment is lost in adjacent text.  A comparative analysis could also be conducted by tokenizing phrases see if additional insight could be collected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install wordcloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(stopwords=stopwords_list,collocations=False)\n",
    "wordcloud.generate(','.join(stopped_tokens))\n",
    "plt.figure(figsize = (12, 12), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"toxic:\")\n",
    "print(train[train['toxic']==1].iloc[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of severe toxic text:\")\n",
    "print('\\n')\n",
    "print(train[train['severe_toxic']==1].iloc[4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of obscene text:\")\n",
    "print('\\n')\n",
    "print(train[train['obscene']==1].iloc[4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the most offensive words in the generalized toxic word cloud:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature of this project brings with it finding uber-offensive text.  For the sake of displaying these terms in  a way to convey sentiment without being overtly offensive, I'm editing some of the most brash. Not sure that this part needs to be in there still.  May delete later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a dictionary of the most offensive words\n",
    "replace = {'fuck': 'f$%!', 'nigger' : 'n*$$#@' ,'nigga':'n#5#*', 'fucking' : 'f*@%!ng',\n",
    "           'faggot':'f@&&*#', 'cunt' : 'c&#^' , 'fag' : 'f@$',\n",
    "           \"'fuck\" : \"'f$%!'\", 'faggots':'f@&&*!$'}\n",
    "\n",
    "#using the 'replace' dictionary above, \n",
    "\n",
    "new_dict = {}\n",
    "for k, v in dict(freq_tox).items():\n",
    "    if k in replace:\n",
    "        key = replace[k]\n",
    "    else:\n",
    "        key = k\n",
    "        \n",
    "    new_dict[key] = v\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_toxic_text = list(new_dict.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edited_toxic_text = str(edited_toxic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timer = fn.Timer()\n",
    "\n",
    "timer.start()\n",
    "edited_toxic_tokens = word_tokenize(edited_toxic_text)\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_tox_clean = WordCloud(stopwords=stopwords_list,collocations=False)\n",
    "wordcloud_tox_clean.generate(','.join(edited_toxic_tokens))\n",
    "plt.figure(figsize = (12, 12), facecolor = None) \n",
    "plt.imshow(wordcloud_tox_clean) \n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(x):\n",
    "    return ['background-color: yellow' if v == x.max() else ''\n",
    "            for v in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crosstab\n",
    "# Since technically a crosstab between all 6 classes is impossible to vizualize, \n",
    "# toxic is most frequent by far and \n",
    "main_col=\"toxic\"\n",
    "corr_mats=[]\n",
    "for other_col in y_train.columns:\n",
    "    confusion_mat = pd.crosstab(y_train[main_col], y_train[other_col])\n",
    "    corr_mats.append(confusion_mat)\n",
    "out = pd.concat(corr_mats,axis=1,keys=y_train)\n",
    "\n",
    "#cell highlighting\n",
    "out = out.style.apply(highlight_max,axis=0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squeaky clean\n",
    "\n",
    "#marking comments without any tags as \"clean\"\n",
    "rowsums=y_train.sum(axis=1)\n",
    "train['clean']=(rowsums==0)\n",
    "train['toxic']=(rowsums!=0)\n",
    "#count number of clean entries\n",
    "train['clean'].sum()\n",
    "print(\"Total comments = \",train['clean'].sum()+train['toxic'].sum())\n",
    "print(\"Total clean comments = \",train['clean'].sum())\n",
    "print(\"Total toxic comments = \",train['toxic'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = fn.Timer()\n",
    "timer.start()\n",
    "tfv = TfidfVectorizer(min_df=200,  max_features=10000, \n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(2,2),\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "X1 = tfv.fit_transform(X_train)\n",
    "features = np.array(tfv.get_feature_names())\n",
    "\n",
    "\n",
    "scores = (X1.toarray()) \n",
    "print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "# Getting top ranking features \n",
    "sums = X1.sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0, col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['term', 'rank']) \n",
    "words = (ranking.sort_values('rank', ascending = False)) \n",
    "print (\"\\n\\nWords : \\n\", words.head(7)) \n",
    "\n",
    "timer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758\n",
    "#https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "#https://pypi.org/project/focal-loss/#:~:text=TensorFlow%20implementation%20of%20focal%20loss%20%5B1%5D%3A%20a%20loss%20function,losses%20functions%20and%20classes%2C%20respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomal classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf = df['comment_text']\n",
    "ydf = df['threat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf_train, Xdf_test, ydf_train, ydf_test = train_test_split(Xdf, ydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(Xdf_train))\n",
    "#try texts_to_matrix\n",
    "list_tokenized_traindf = tokenizer.texts_to_sequences(Xdf_train)\n",
    "list_tokenized_testdf = tokenizer.texts_to_sequences(Xdf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcomments = [len(comment) for comment in list_tokenized_traindf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "X_t_df = pad_sequences(list_tokenized_traindf, maxlen=maxlen)\n",
    "X_te_df = pad_sequences(list_tokenized_testdf, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_smalldf = Sequential()\n",
    "embedding_size = 1100\n",
    "\n",
    "rnn_smalldf.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_smalldf.add(LSTM(20, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_smalldf.add(GlobalMaxPool1D())\n",
    "rnn_smalldf.add(Dropout(0.1))  \n",
    "rnn_smalldf.add(Dense(10, kernel_regularizer=regularizers.l2(.0001),activation='relu'))\n",
    "rnn_smalldf.add(Dropout(0.1))\n",
    "rnn_smalldf.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_smalldf.compile(loss=BinaryFocalLoss(gamma=2),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING ⏰ 21 Min+ RunTime ⏰\n",
    "#fit the model\n",
    "timer = fn.Timer()\n",
    "timer.start()\n",
    "history_smalldf = rnn_smalldf.fit(X_t_df, ydf_train, epochs=20, batch_size=300, \n",
    "                        callbacks=early_stopping, validation_split=0.3)\n",
    "timer = timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(history_smalldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = ['comment_text', 'threat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba_rnn_smalldf = rnn_smalldf.predict(X_te_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_preds_rnn_smalldf = pd.DataFrame(np.round(y_hat_test_proba_rnn_smalldf), columns=target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.class_report_model(ydf_train, ydf_test, y_preds_rnn_smalldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def calc_loss(y_true, y_pred):\n",
    "    return np.mean([log_loss(y_true[:, i], y_pred[:, i]) \n",
    "                    for i in range(y_true.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.528px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
